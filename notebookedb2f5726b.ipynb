{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-02T04:38:34.589555Z","iopub.status.busy":"2024-01-02T04:38:34.589172Z","iopub.status.idle":"2024-01-02T04:38:34.594206Z","shell.execute_reply":"2024-01-02T04:38:34.593147Z","shell.execute_reply.started":"2024-01-02T04:38:34.589524Z"},"trusted":true},"outputs":[],"source":["import cupy, cudf # GPU LIBRARIES\n","import numpy as np, pandas as pd # CPU LIBRARIES\n","import matplotlib.pyplot as plt, gc, os\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["**What are Transformers?\n","**\n","Transformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP). They were first introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017, and have since become the go-to model for a wide range of NLP tasks, including machine translation, text summarization, question answering, and text generation.\n","\n","*Transformers are NLP superstars, replacing older models like RNNs.\n","*They ditch sequential processing for self-attention, considering all words at once.\n","*This \"attention\" lets them learn word relationships and long-range context.\n","*Faster, more accurate, and better at complex tasks like translation and summarization.\n","*Pre-trained on massive datasets, they adapt easily to specific tasks.\n","*From generating poems to answering questions, Transformers are all-around NLP champs.\n","*The future of NLP? Buckle up, Transformers are leading the ride.\n","Source: Bard"]},{"cell_type":"markdown","metadata":{},"source":["# **Load Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:34.601734Z","iopub.status.busy":"2024-01-02T04:38:34.601479Z","iopub.status.idle":"2024-01-02T04:38:46.430898Z","shell.execute_reply":"2024-01-02T04:38:46.42975Z","shell.execute_reply.started":"2024-01-02T04:38:34.601711Z"},"trusted":true},"outputs":[],"source":["pip install keras-utils\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:46.43305Z","iopub.status.busy":"2024-01-02T04:38:46.432768Z","iopub.status.idle":"2024-01-02T04:38:46.443863Z","shell.execute_reply":"2024-01-02T04:38:46.442783Z","shell.execute_reply.started":"2024-01-02T04:38:46.433024Z"},"trusted":true},"outputs":[],"source":["# import libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.pyplot import xticks\n","from nltk.corpus import stopwords\n","import nltk\n","import re\n","from nltk.stem import WordNetLemmatizer\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.util import ngrams\n","from collections import defaultdict\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from keras.utils import plot_model\n","\n","#from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n","import tensorflow as tf\n","from sklearn.metrics import f1_score\n","from wordcloud import WordCloud,STOPWORDS\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from keras.preprocessing.sequence import pad_sequences\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense,Flatten,Embedding,Activation,Dropout\n","from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:46.445707Z","iopub.status.busy":"2024-01-02T04:38:46.445328Z","iopub.status.idle":"2024-01-02T04:38:46.458608Z","shell.execute_reply":"2024-01-02T04:38:46.457759Z","shell.execute_reply.started":"2024-01-02T04:38:46.445669Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"]},{"cell_type":"markdown","metadata":{},"source":["# Building Transformer block as a layer\n","# "]},{"cell_type":"markdown","metadata":{},"source":["1. It's a self-contained unit that captures relationships between words and learns complex patterns using multi-head attention and feed-forward networks.\n","2. Layer normalization and residual connections stabilize training and enhance information flow.\n","3. Multiple blocks are stacked to create powerful language models, with each block refining contextual understanding.\n","4. This modular structure offers flexibility, reusability, and easier understanding of model behavior."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:46.461045Z","iopub.status.busy":"2024-01-02T04:38:46.460634Z","iopub.status.idle":"2024-01-02T04:38:46.469868Z","shell.execute_reply":"2024-01-02T04:38:46.468999Z","shell.execute_reply.started":"2024-01-02T04:38:46.461017Z"},"trusted":true},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{},"source":["# ****Building Implement embedding layer**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:46.471199Z","iopub.status.busy":"2024-01-02T04:38:46.470909Z","iopub.status.idle":"2024-01-02T04:38:46.484147Z","shell.execute_reply":"2024-01-02T04:38:46.483357Z","shell.execute_reply.started":"2024-01-02T04:38:46.471175Z"},"trusted":true},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"]},{"cell_type":"markdown","metadata":{},"source":["# ******Downloading dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:46.486142Z","iopub.status.busy":"2024-01-02T04:38:46.485275Z","iopub.status.idle":"2024-01-02T04:38:52.483612Z","shell.execute_reply":"2024-01-02T04:38:52.482806Z","shell.execute_reply.started":"2024-01-02T04:38:46.486108Z"},"trusted":true},"outputs":[],"source":["vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 200  # Only consider the first 200 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{},"source":["# ****Create classifier model with transformer layer**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:38:52.484997Z","iopub.status.busy":"2024-01-02T04:38:52.484711Z","iopub.status.idle":"2024-01-02T04:38:52.693177Z","shell.execute_reply":"2024-01-02T04:38:52.692412Z","shell.execute_reply.started":"2024-01-02T04:38:52.484972Z"},"trusted":true},"outputs":[],"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# ****Train and Evaluate**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:40:48.481405Z","iopub.status.busy":"2024-01-02T04:40:48.480644Z","iopub.status.idle":"2024-01-02T04:42:21.941428Z","shell.execute_reply":"2024-01-02T04:42:21.940498Z","shell.execute_reply.started":"2024-01-02T04:40:48.481372Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=3, validation_data=(x_val, y_val)\n",")"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
